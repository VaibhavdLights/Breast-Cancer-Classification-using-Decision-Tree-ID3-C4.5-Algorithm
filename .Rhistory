}
# Find the best attribute to split on
split_attribute <- best_split_attribute(data)
# Initialize an empty tree
tree <- list(attribute = split_attribute, children = list())
# Split the data and build subtrees
attribute_values <- unique(data[, split_attribute])
for (value in attribute_values) {
subset_data <- data[data[, split_attribute] == value, ]
subset_data <- subset_data[, colnames(subset_data) != split_attribute, drop = FALSE]
tree$children[[as.character(value)]] <- build_tree(subset_data)
}
return(tree)
}
# Build the decision tree
tree <- build_tree(data)
# You can now use the 'tree' to make predictions on new data
# Function to make predictions using the built tree
# Function to make predictions using the built tree
predict_example <- function(tree, example) {
if (length(tree) == 1) {
return(tree)  # Return the predicted class
} else {
attribute <- tree$attribute
attribute_value <- example[attribute]
if (attribute_value %in% names(tree$children)) {
subtree <- tree$children[[as.character(attribute_value)]]
return(predict_example(subtree, example))
} else {
# Handle the case where the attribute_value is not found in the tree
# You can return a default value or handle this situation as needed.
# For example, return the majority class of the current node.
return(names(sort(table(tree$target_column), decreasing = TRUE)[1]))
}
}
}
# Make a prediction
prediction <- predict_tree(tree, data)
# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)
data <- read.csv("cleaned_data.csv")
# Define a function to calculate entropy
entropy <- function(data) {
if (length(data) == 0) return(0)
proportions <- table(data) / length(data)
-sum(proportions * log2(proportions))
}
# Define a function to calculate information gain for a given attribute
information_gain <- function(data, attribute) {
total_entropy <- entropy(data$target_column)
split_entropy <- 0
attribute_values <- unique(data$attribute)
for (value in attribute_values) {
subset_data <- data[data$attribute == value, ]
weight <- nrow(subset_data) / nrow(data)
split_entropy <- split_entropy + weight * entropy(subset_data$target_column)
}
total_entropy - split_entropy
}
# Define a function to select the best attribute to split on
best_split_attribute <- function(data) {
attributes <- colnames(data)[!colnames(data) %in% "target_column"]
info_gains <- sapply(attributes, function(attr) information_gain(data, data[, attr]))
attributes[which.max(info_gains)]
}
# Define a function to build the decision tree recursively
build_tree <- function(data) {
if (length(unique(data$target_column)) == 1) {
# If all instances have the same class, return a leaf node with that class
return(data$target_column[1])
}
if (ncol(data) == 1) {
# If there are no attributes left to split on, return the majority class
return(names(sort(table(data$target_column), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
split_attribute <- best_split_attribute(data)
# Initialize an empty tree
tree <- list(attribute = split_attribute, children = list())
# Split the data and build subtrees
attribute_values <- unique(data[, split_attribute])
for (value in attribute_values) {
subset_data <- data[data[, split_attribute] == value, ]
subset_data <- subset_data[, colnames(subset_data) != split_attribute, drop = FALSE]
tree$children[[as.character(value)]] <- build_tree(subset_data)
}
return(tree)
}
# Build the decision tree
tree <- build_tree(data)
# Function to make predictions using the built tree
predict_example <- function(tree, example) {
if (length(tree) == 1) {
cat("Reached leaf node. Predicted class:", tree, "\n")
return(tree)  # Return the predicted class
} else {
attribute <- tree$attribute
attribute_value <- example[attribute]
cat("Current attribute:", attribute, "\n")
cat("Current attribute value:", attribute_value, "\n")
if (is.null(tree$children)) {
cat("No children for this attribute.\n")
return(NULL)
}
if (attribute_value %in% names(tree$children)) {
cat("Attribute value found in children.\n")
subtree <- tree$children[[as.character(attribute_value)]]
return(predict_example(subtree, example))
} else {
cat("Attribute value not found in children.\n")
# Handle the case where the attribute_value is not found in the tree
# You can return a default value or handle this situation as needed.
# For example, return the majority class of the current node.
return(names(sort(table(tree$target_column), decreasing = TRUE)[1]))
}
}
}
# Make a prediction
prediction <- predict_tree(tree, data)
# Load your breast cancer dataset
data <- read.csv("BreastCancer.csv")
# Define a function to calculate entropy
entropy <- function(p) {
-sum(p * log2(p))
}
# Define a function to calculate information gain
information_gain <- function(data, attribute, target) {
# Calculate the entropy of the entire dataset
total_entropy <- entropy(table(data[[target]]) / nrow(data))
# Calculate the weighted average of entropies after splitting on the attribute
weighted_entropy <- 0
attribute_values <- unique(data[[attribute]])
for (value in attribute_values) {
subset_data <- data[data[[attribute]] == value, ]
weight <- nrow(subset_data) / nrow(data)
weighted_entropy <- weighted_entropy + weight * entropy(table(subset_data[[target]]) / nrow(subset_data))
}
# Calculate information gain
information_gain <- total_entropy - weighted_entropy
return(information_gain)
}
# Define a function to find the best attribute to split on
find_best_split_attribute <- function(data, target) {
attributes <- colnames(data)
attributes <- attributes[attributes != target]
best_attribute <- attributes[1]
best_gain <- information_gain(data, attributes[1], target)
for (attribute in attributes) {
gain <- information_gain(data, attribute, target)
if (gain > best_gain) {
best_attribute <- attribute
best_gain <- gain
}
}
return(best_attribute)
}
# Define a recursive function to build the decision tree
build_decision_tree <- function(data, target) {
if (length(unique(data[[target]])) == 1) {
# All instances have the same class, create a leaf node
return(data[[target]][1])
}
if (length(colnames(data)) == 1) {
# No attributes left to split on, return the majority class
return(names(sort(table(data[[target]]), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
best_attribute <- find_best_split_attribute(data, target)
# Create a decision node for the best attribute
tree <- list()
tree$attribute <- best_attribute
tree$children <- list()
# Recurse on the sublists obtained by splitting on the best attribute
attribute_values <- unique(data[[best_attribute]])
for (value in attribute_values) {
subset_data <- data[data[[best_attribute]] == value, ]
if (nrow(subset_data) == 0) {
# No instances for this branch, return the majority class of the parent node
tree$children[[as.character(value)]] <- names(sort(table(data[[target]]), decreasing = TRUE)[1])
} else {
tree$children[[as.character(value)]] <- build_decision_tree(subset_data, target)
}
}
return(tree)
}
# Build the decision tree
decision_tree <- build_decision_tree(data, "Class")
# Print or manipulate the decision tree as needed
print(decision_tree)
# Install and load the 'igraph' library
library(igraph)
# Function to create a graph from the decision tree
create_tree_graph <- function(tree) {
g <- graph.empty(directed = TRUE)
create_node <- function(node, parent = NULL) {
nodeName <- node$attribute
if (!is.null(parent)) {
g <- add.edges(g, c(parent, nodeName))
}
for (child in names(node$children)) {
g <- create_node(node$children[[child]], parent = nodeName)
}
return(g)
}
return(create_node(tree))
}
# Create a graph from the decision tree
tree_graph <- create_tree_graph(decision_tree)
# Load your breast cancer dataset
data <- read.csv("BreastCancer.csv")
# Define a function to calculate entropy
entropy <- function(p) {
-sum(p * log2(p))
}
# Define a function to calculate information gain
information_gain <- function(data, attribute, target) {
# Calculate the entropy of the entire dataset
total_entropy <- entropy(table(data[[target]]) / nrow(data))
# Calculate the weighted average of entropies after splitting on the attribute
weighted_entropy <- 0
attribute_values <- unique(data[[attribute]])
for (value in attribute_values) {
subset_data <- data[data[[attribute]] == value, ]
weight <- nrow(subset_data) / nrow(data)
weighted_entropy <- weighted_entropy + weight * entropy(table(subset_data[[target]]) / nrow(subset_data))
}
# Calculate information gain
information_gain <- total_entropy - weighted_entropy
return(information_gain)
}
# Define a function to find the best attribute to split on
find_best_split_attribute <- function(data, target) {
attributes <- colnames(data)
attributes <- attributes[attributes != target]
best_attribute <- attributes[1]
best_gain <- information_gain(data, attributes[1], target)
for (attribute in attributes) {
gain <- information_gain(data, attribute, target)
if (gain > best_gain) {
best_attribute <- attribute
best_gain <- gain
}
}
return(best_attribute)
}
# Define a recursive function to build the decision tree
build_decision_tree <- function(data, target) {
if (length(unique(data[[target]])) == 1) {
# All instances have the same class, create a leaf node
return(data[[target]][1])
}
if (length(colnames(data)) == 1) {
# No attributes left to split on, return the majority class
return(names(sort(table(data[[target]]), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
best_attribute <- find_best_split_attribute(data, target)
# Create a decision node for the best attribute
tree <- list()
tree$attribute <- best_attribute
tree$children <- list()
# Recurse on the sublists obtained by splitting on the best attribute
attribute_values <- unique(data[[best_attribute]])
for (value in attribute_values) {
subset_data <- data[data[[best_attribute]] == value, ]
if (nrow(subset_data) == 0) {
# No instances for this branch, return the majority class of the parent node
tree$children[[as.character(value)]] <- names(sort(table(data[[target]]), decreasing = TRUE)[1])
} else {
tree$children[[as.character(value)]] <- build_decision_tree(subset_data, target)
}
}
return(tree)
}
# Build the decision tree
decision_tree <- build_decision_tree(data, "Class")
# Install and load the 'igraph' library
library(igraph)
# Function to create a graph from the decision tree
create_tree_graph <- function(tree) {
g <- graph.empty(directed = TRUE)
create_node <- function(node, parent = NULL) {
nodeName <- node$attribute
if (!is.null(parent)) {
g <- add.edges(g, c(parent, nodeName))
}
for (child in names(node$children)) {
g <- create_node(node$children[[child]], parent = nodeName)
}
return(g)
}
return(create_node(tree))
}
# Create a graph from the decision tree
tree_graph <- create_tree_graph(decision_tree)
# Load necessary libraries
library(party)
library(caret)
library(e1071)
library(RColorBrewer)
# Load your breast cancer dataset from a CSV file
data <- read.csv("breast_cancer_data.csv")
# Load necessary libraries
library(party)
library(caret)
library(e1071)
library(RColorBrewer)
# Load your breast cancer dataset from a CSV file
data <- read.csv("BreastCancer.csv")
# Define a function to calculate information gain
information_gain <- function(data, attribute, class) {
# Calculate entropy for the entire dataset
entropy_total <- entropy(data$class)
# Calculate the weighted average entropy after splitting on the attribute
weighted_entropy <- 0
unique_values <- unique(data[[attribute]])
for (value in unique_values) {
subset_data <- data[data[[attribute]] == value, ]
weight <- nrow(subset_data) / nrow(data)
weighted_entropy <- weighted_entropy + weight * entropy(subset_data$class)
}
# Calculate information gain
information_gain <- entropy_total - weighted_entropy
return(information_gain)
}
# Define a function to calculate entropy
entropy <- function(class) {
probabilities <- table(class) / length(class)
entropy <- -sum(probabilities * log2(probabilities + 1e-10))
return(entropy)
}
# Define a recursive function to build the decision tree
build_tree <- function(data, depth) {
# Check for base cases
if (depth == 0 || length(unique(data$class)) == 1) {
return(leaf(data$class))
}
# Initialize variables to find the best attribute
best_attribute <- NULL
best_information_gain <- -Inf
# Find the attribute with the highest information gain
for (attribute in names(data)[names(data) != "class"]) {
gain <- information_gain(data, attribute, data$class)
if (gain > best_information_gain) {
best_information_gain <- gain
best_attribute <- attribute
}
}
# Create a decision node
node <- split_node(data, best_attribute)
# Recurse on the sublists obtained by splitting on the best attribute
unique_values <- unique(data[[best_attribute]])
for (value in unique_values) {
subset_data <- data[data[[best_attribute]] == value, ]
child <- build_tree(subset_data, depth - 1)
add_child(node, value, child)
}
return(node)
}
# Define a function to create a leaf node
leaf <- function(class) {
return(paste("Leaf:", names(sort(table(class), decreasing = TRUE)[1])))
}
# Define a function to create a split node
split_node <- function(data, attribute) {
node <- list()
node$attribute <- attribute
node$children <- list()
return(node)
}
# Define a function to add a child node to a split node
add_child <- function(node, value, child) {
node$children[[as.character(value)]] <- child
}
# Build the decision tree
tree <- build_tree(data, depth = 3) # You can adjust the depth as needed
# Print the decision tree
print(tree)
# You can plot the tree with different packages like 'party' or 'rpart.plot'
# Make predictions using the decision tree
predictions <- sapply(1:nrow(data), function(i) predict_tree(tree, data[i, ]))
# Create a confusion matrix
confusion_matrix <- table(Actual = data$class, Predicted = predictions)
library(C50)  # This library provides C5.0 algorithm (C4.5 variant) for decision tree classification
library(caret) # For data splitting
# Load your breast cancer dataset as a DataFrame (replace 'your_breast_cancer_data.csv' with your actual dataset file)
data <- read.csv("BreastCancer.csv")
# Define the target column
target_column <- "Class"
# Handle missing values (remove rows with missing data)
data <- na.omit(data)
# Encode categorical variables (if any)
# Assuming 'Class' is the target column and other categorical columns need encoding
categorical_cols <- c("CategoricalColumn1", "CategoricalColumn2")
data <- data.frame(lapply(data, as.factor))  # Convert all columns to factors
data <- dummyVars(target_column ~ ., data = data[, -which(names(data) == target_column)], fullRank = TRUE)  # Perform one-hot encoding
library(C50)  # This library provides C5.0 algorithm (C4.5 variant) for decision tree classification
library(caret) # For data splitting
data <- read.csv("BreastCancer.csv")
data <- na.omit(data)
# Define the target column
target_column <- "Class"
# Split the data into training and testing sets (70% for training, 30% for testing)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(data$Class, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
# Remove the target column from the data frames
train_data <- train_data[, -which(names(train_data) == target_column)]
test_data <- test_data[, -which(names(test_data) == target_column)]
# Train a C5.0 decision tree model
c5_model <- C5.0(train_data, train_data$Class)
library(C50)  # This library provides C5.0 algorithm (C4.5 variant) for decision tree classification
library(caret) # For data splitting
data <- read.csv("BreastCancer.csv")
data <- na.omit(data)
# Define the target column
target_column <- "Class"
# Split the data into training and testing sets (70% for training, 30% for testing)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(data$Class, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
# Remove the target column from the data frames
train_data <- train_data[, -which(names(train_data) == target_column)]
test_data <- test_data[, -which(names(test_data) == target_column)]
data$Class <- as.factor(data$Class)
levels(data$Class) <- c("0", "1")
# Train a C5.0 decision tree model
c5_model <- C5.0(train_data, train_data$Class)
library(C50)  # This library provides C5.0 algorithm (C4.5 variant) for decision tree classification
library(caret) # For data splitting
data <- read.csv("BreastCancer.csv")
data <- na.omit(data)
# Define the target column
target_column <- "Class"
# Split the data into training and testing sets (70% for training, 30% for testing)
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(data$Class, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
# Remove the target column from the data frames
train_data <- train_data[, -which(names(train_data) == target_column)]
test_data <- test_data[, -which(names(test_data) == target_column)]
data$Class <- as.factor(data$Class)
# Train a C5.0 decision tree model
c5_model <- C5.0(train_data, train_data$Class)
library(C50)
# Step 2: Load the breast cancer dataset from a CSV file
breast_cancer_data <- read.csv("your_breast_cancer_data.csv")
library(C50)
# Step 2: Load the breast cancer dataset from a CSV file
breast_cancer_data <- read.csv("BreastCancer.csv")
# Step 3: Check the structure of the dataset
str(breast_cancer_data)
# Step 4: Create the target variable if it doesn't already exist
breast_cancer_data$Class <- as.factor(breast_cancer_data$diagnosis)
library(C50)
# Step 2: Load the breast cancer dataset from a CSV file
breast_cancer_data <- read.csv("BreastCancer.csv")
# Step 3: Check the structure of the dataset
str(breast_cancer_data)
# Step 4: Create the target variable if it doesn't already exist
if (!"Class" %in% colnames(breast_cancer_data)) {
breast_cancer_data$Class <- as.factor(breast_cancer_data$diagnosis)
}
# Step 5: Split the dataset into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(breast_cancer_data), 0.7 * nrow(breast_cancer_data))  # 70% for training
train_data <- breast_cancer_data[train_indices, ]
test_data <- breast_cancer_data[-train_indices, ]
# Step 6: Build the C5.0 decision tree model
c5.0_model <- C5.0(Class ~ ., data = train_data)
library(C50)
# Step 2: Load the breast cancer dataset from a CSV file
breast_cancer_data <- read.csv("BreastCancer.csv")
# Step 3: Check the structure of the dataset
str(breast_cancer_data)
# Step 4: Ensure that the "Class" column is a factor
breast_cancer_data$Class <- as.factor(breast_cancer_data$Class)
# Step 5: Split the dataset into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(breast_cancer_data), 0.7 * nrow(breast_cancer_data))  # 70% for training
train_data <- breast_cancer_data[train_indices, ]
test_data <- breast_cancer_data[-train_indices, ]
# Step 6: Build the C5.0 decision tree model
c5.0_model <- C5.0(Class ~ ., data = train_data)
# Step 7: Summarize the model
summary(c5.0_model)
# Step 8: Predict on the test data
predictions <- predict(c5.0_model, newdata = test_data)
# Step 9: Evaluate the model's performance
accuracy <- sum(predictions == test_data$Class) / nrow(test_data)
cat("Accuracy:", accuracy, "\n")
# Load necessary packages for performance evaluation
library(caret)
library(e1071)
# Predict on the test data
predictions <- predict(c5.0_model, newdata = test_data)
# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$Class)
# Extract the relevant performance measures
accuracy <- confusion_matrix$overall["Accuracy"]
f1_score <- confusion_matrix$byClass["F1"]
precision <- confusion_matrix$byClass["Pos Pred Value"]
recall <- confusion_matrix$byClass["Sensitivity"]
# Print the results
cat("Confusion Matrix:\n")
print(confusion_matrix$table)
cat("\nPredictive Accuracy:", accuracy, "\n")
cat("F1-Score:", f1_score, "\n")
cat("Precision:", precision, "\n")
cat("Recall (Sensitivity):", recall, "\n")
