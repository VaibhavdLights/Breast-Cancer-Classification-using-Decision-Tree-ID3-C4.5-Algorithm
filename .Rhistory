# Split the data and build subtrees
attribute_values <- unique(data[, split_attribute])
for (value in attribute_values) {
subset_data <- data[data[, split_attribute] == value, ]
subset_data <- subset_data[, colnames(subset_data) != split_attribute, drop = FALSE]
tree$children[[as.character(value)]] <- build_tree(subset_data)
}
return(tree)
}
# Build the decision tree
tree <- build_tree(data)
# You can now use the 'tree' to make predictions on new data
predict_tree <- function(tree, new_data) {
if (is.character(tree)) {
# If the current node is a leaf node (class label), return it
return(tree)
} else {
# If the current node is an internal node, follow the tree
attribute_value <- new_data[[tree$attribute]]
if (is.null(tree$children[[as.character(attribute_value)]])) {
# Handle cases where the attribute value is not in the tree
return("Unknown")
} else {
next_node <- tree$children[[as.character(attribute_value)]]
return(predict_tree(next_node, new_data))
}
}
}
# Make a prediction
prediction <- predict_tree(tree, data)
# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)
data <- read.csv("cleaned_data.csv")
# Define a function to calculate entropy
entropy <- function(data) {
if (length(data) == 0) return(0)
proportions <- table(data) / length(data)
-sum(proportions * log2(proportions))
}
# Define a function to calculate information gain for a given attribute
information_gain <- function(data, attribute) {
total_entropy <- entropy(data$target_column)
split_entropy <- 0
attribute_values <- unique(data$attribute)
for (value in attribute_values) {
subset_data <- data[data$attribute == value, ]
weight <- nrow(subset_data) / nrow(data)
split_entropy <- split_entropy + weight * entropy(subset_data$target_column)
}
total_entropy - split_entropy
}
# Define a function to select the best attribute to split on
best_split_attribute <- function(data) {
attributes <- colnames(data)[!colnames(data) %in% "target_column"]
info_gains <- sapply(attributes, function(attr) information_gain(data, data[, attr]))
attributes[which.max(info_gains)]
}
# Define a function to build the decision tree recursively
build_tree <- function(data) {
if (length(unique(data$target_column)) == 1) {
# If all instances have the same class, return a leaf node with that class
return(data$target_column[1])
}
if (ncol(data) == 1) {
# If there are no attributes left to split on, return the majority class
return(names(sort(table(data$target_column), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
split_attribute <- best_split_attribute(data)
# Initialize an empty tree
tree <- list(attribute = split_attribute, children = list())
# Split the data and build subtrees
attribute_values <- unique(data[, split_attribute])
for (value in attribute_values) {
subset_data <- data[data[, split_attribute] == value, ]
subset_data <- subset_data[, colnames(subset_data) != split_attribute, drop = FALSE]
tree$children[[as.character(value)]] <- build_tree(subset_data)
}
return(tree)
}
# Build the decision tree
tree <- build_tree(data)
# You can now use the 'tree' to make predictions on new data
# Function to make predictions using the built tree
predict_example <- function(tree, example) {
if (length(tree) == 1) {
return(tree)  # Return the predicted class
} else {
attribute <- tree$attribute
attribute_value <- example[attribute]
subtree <- tree$children[[as.character(attribute_value)]]
return(predict_example(subtree, example))
}
}
# Make a prediction
prediction <- predict_tree(tree, data)
# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)
data <- read.csv("cleaned_data.csv")
# Define a function to calculate entropy
entropy <- function(data) {
if (length(data) == 0) return(0)
proportions <- table(data) / length(data)
-sum(proportions * log2(proportions))
}
# Define a function to calculate information gain for a given attribute
information_gain <- function(data, attribute) {
total_entropy <- entropy(data$target_column)
split_entropy <- 0
attribute_values <- unique(data$attribute)
for (value in attribute_values) {
subset_data <- data[data$attribute == value, ]
weight <- nrow(subset_data) / nrow(data)
split_entropy <- split_entropy + weight * entropy(subset_data$target_column)
}
total_entropy - split_entropy
}
# Define a function to select the best attribute to split on
best_split_attribute <- function(data) {
attributes <- colnames(data)[!colnames(data) %in% "target_column"]
info_gains <- sapply(attributes, function(attr) information_gain(data, data[, attr]))
attributes[which.max(info_gains)]
}
# Define a function to build the decision tree recursively
build_tree <- function(data) {
if (length(unique(data$target_column)) == 1) {
# If all instances have the same class, return a leaf node with that class
return(data$target_column[1])
}
if (ncol(data) == 1) {
# If there are no attributes left to split on, return the majority class
return(names(sort(table(data$target_column), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
split_attribute <- best_split_attribute(data)
# Initialize an empty tree
tree <- list(attribute = split_attribute, children = list())
# Split the data and build subtrees
attribute_values <- unique(data[, split_attribute])
for (value in attribute_values) {
subset_data <- data[data[, split_attribute] == value, ]
subset_data <- subset_data[, colnames(subset_data) != split_attribute, drop = FALSE]
tree$children[[as.character(value)]] <- build_tree(subset_data)
}
return(tree)
}
# Build the decision tree
tree <- build_tree(data)
# You can now use the 'tree' to make predictions on new data
# Function to make predictions using the built tree
# Function to make predictions using the built tree
predict_example <- function(tree, example) {
if (length(tree) == 1) {
return(tree)  # Return the predicted class
} else {
attribute <- tree$attribute
attribute_value <- example[attribute]
if (attribute_value %in% names(tree$children)) {
subtree <- tree$children[[as.character(attribute_value)]]
return(predict_example(subtree, example))
} else {
# Handle the case where the attribute_value is not found in the tree
# You can return a default value or handle this situation as needed.
# For example, return the majority class of the current node.
return(names(sort(table(tree$target_column), decreasing = TRUE)[1]))
}
}
}
# Make a prediction
prediction <- predict_tree(tree, data)
# Load your dataset (replace 'your_dataset.csv' with your actual dataset file)
data <- read.csv("cleaned_data.csv")
# Define a function to calculate entropy
entropy <- function(data) {
if (length(data) == 0) return(0)
proportions <- table(data) / length(data)
-sum(proportions * log2(proportions))
}
# Define a function to calculate information gain for a given attribute
information_gain <- function(data, attribute) {
total_entropy <- entropy(data$target_column)
split_entropy <- 0
attribute_values <- unique(data$attribute)
for (value in attribute_values) {
subset_data <- data[data$attribute == value, ]
weight <- nrow(subset_data) / nrow(data)
split_entropy <- split_entropy + weight * entropy(subset_data$target_column)
}
total_entropy - split_entropy
}
# Define a function to select the best attribute to split on
best_split_attribute <- function(data) {
attributes <- colnames(data)[!colnames(data) %in% "target_column"]
info_gains <- sapply(attributes, function(attr) information_gain(data, data[, attr]))
attributes[which.max(info_gains)]
}
# Define a function to build the decision tree recursively
build_tree <- function(data) {
if (length(unique(data$target_column)) == 1) {
# If all instances have the same class, return a leaf node with that class
return(data$target_column[1])
}
if (ncol(data) == 1) {
# If there are no attributes left to split on, return the majority class
return(names(sort(table(data$target_column), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
split_attribute <- best_split_attribute(data)
# Initialize an empty tree
tree <- list(attribute = split_attribute, children = list())
# Split the data and build subtrees
attribute_values <- unique(data[, split_attribute])
for (value in attribute_values) {
subset_data <- data[data[, split_attribute] == value, ]
subset_data <- subset_data[, colnames(subset_data) != split_attribute, drop = FALSE]
tree$children[[as.character(value)]] <- build_tree(subset_data)
}
return(tree)
}
# Build the decision tree
tree <- build_tree(data)
# Function to make predictions using the built tree
predict_example <- function(tree, example) {
if (length(tree) == 1) {
cat("Reached leaf node. Predicted class:", tree, "\n")
return(tree)  # Return the predicted class
} else {
attribute <- tree$attribute
attribute_value <- example[attribute]
cat("Current attribute:", attribute, "\n")
cat("Current attribute value:", attribute_value, "\n")
if (is.null(tree$children)) {
cat("No children for this attribute.\n")
return(NULL)
}
if (attribute_value %in% names(tree$children)) {
cat("Attribute value found in children.\n")
subtree <- tree$children[[as.character(attribute_value)]]
return(predict_example(subtree, example))
} else {
cat("Attribute value not found in children.\n")
# Handle the case where the attribute_value is not found in the tree
# You can return a default value or handle this situation as needed.
# For example, return the majority class of the current node.
return(names(sort(table(tree$target_column), decreasing = TRUE)[1]))
}
}
}
# Make a prediction
prediction <- predict_tree(tree, data)
# Load your breast cancer dataset
data <- read.csv("BreastCancer.csv")
# Define a function to calculate entropy
entropy <- function(p) {
-sum(p * log2(p))
}
# Define a function to calculate information gain
information_gain <- function(data, attribute, target) {
# Calculate the entropy of the entire dataset
total_entropy <- entropy(table(data[[target]]) / nrow(data))
# Calculate the weighted average of entropies after splitting on the attribute
weighted_entropy <- 0
attribute_values <- unique(data[[attribute]])
for (value in attribute_values) {
subset_data <- data[data[[attribute]] == value, ]
weight <- nrow(subset_data) / nrow(data)
weighted_entropy <- weighted_entropy + weight * entropy(table(subset_data[[target]]) / nrow(subset_data))
}
# Calculate information gain
information_gain <- total_entropy - weighted_entropy
return(information_gain)
}
# Define a function to find the best attribute to split on
find_best_split_attribute <- function(data, target) {
attributes <- colnames(data)
attributes <- attributes[attributes != target]
best_attribute <- attributes[1]
best_gain <- information_gain(data, attributes[1], target)
for (attribute in attributes) {
gain <- information_gain(data, attribute, target)
if (gain > best_gain) {
best_attribute <- attribute
best_gain <- gain
}
}
return(best_attribute)
}
# Define a recursive function to build the decision tree
build_decision_tree <- function(data, target) {
if (length(unique(data[[target]])) == 1) {
# All instances have the same class, create a leaf node
return(data[[target]][1])
}
if (length(colnames(data)) == 1) {
# No attributes left to split on, return the majority class
return(names(sort(table(data[[target]]), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
best_attribute <- find_best_split_attribute(data, target)
# Create a decision node for the best attribute
tree <- list()
tree$attribute <- best_attribute
tree$children <- list()
# Recurse on the sublists obtained by splitting on the best attribute
attribute_values <- unique(data[[best_attribute]])
for (value in attribute_values) {
subset_data <- data[data[[best_attribute]] == value, ]
if (nrow(subset_data) == 0) {
# No instances for this branch, return the majority class of the parent node
tree$children[[as.character(value)]] <- names(sort(table(data[[target]]), decreasing = TRUE)[1])
} else {
tree$children[[as.character(value)]] <- build_decision_tree(subset_data, target)
}
}
return(tree)
}
# Build the decision tree
decision_tree <- build_decision_tree(data, "Class")
# Print or manipulate the decision tree as needed
print(decision_tree)
# Install and load the 'igraph' library
library(igraph)
# Function to create a graph from the decision tree
create_tree_graph <- function(tree) {
g <- graph.empty(directed = TRUE)
create_node <- function(node, parent = NULL) {
nodeName <- node$attribute
if (!is.null(parent)) {
g <- add.edges(g, c(parent, nodeName))
}
for (child in names(node$children)) {
g <- create_node(node$children[[child]], parent = nodeName)
}
return(g)
}
return(create_node(tree))
}
# Create a graph from the decision tree
tree_graph <- create_tree_graph(decision_tree)
# Load your breast cancer dataset
data <- read.csv("BreastCancer.csv")
# Define a function to calculate entropy
entropy <- function(p) {
-sum(p * log2(p))
}
# Define a function to calculate information gain
information_gain <- function(data, attribute, target) {
# Calculate the entropy of the entire dataset
total_entropy <- entropy(table(data[[target]]) / nrow(data))
# Calculate the weighted average of entropies after splitting on the attribute
weighted_entropy <- 0
attribute_values <- unique(data[[attribute]])
for (value in attribute_values) {
subset_data <- data[data[[attribute]] == value, ]
weight <- nrow(subset_data) / nrow(data)
weighted_entropy <- weighted_entropy + weight * entropy(table(subset_data[[target]]) / nrow(subset_data))
}
# Calculate information gain
information_gain <- total_entropy - weighted_entropy
return(information_gain)
}
# Define a function to find the best attribute to split on
find_best_split_attribute <- function(data, target) {
attributes <- colnames(data)
attributes <- attributes[attributes != target]
best_attribute <- attributes[1]
best_gain <- information_gain(data, attributes[1], target)
for (attribute in attributes) {
gain <- information_gain(data, attribute, target)
if (gain > best_gain) {
best_attribute <- attribute
best_gain <- gain
}
}
return(best_attribute)
}
# Define a recursive function to build the decision tree
build_decision_tree <- function(data, target) {
if (length(unique(data[[target]])) == 1) {
# All instances have the same class, create a leaf node
return(data[[target]][1])
}
if (length(colnames(data)) == 1) {
# No attributes left to split on, return the majority class
return(names(sort(table(data[[target]]), decreasing = TRUE)[1]))
}
# Find the best attribute to split on
best_attribute <- find_best_split_attribute(data, target)
# Create a decision node for the best attribute
tree <- list()
tree$attribute <- best_attribute
tree$children <- list()
# Recurse on the sublists obtained by splitting on the best attribute
attribute_values <- unique(data[[best_attribute]])
for (value in attribute_values) {
subset_data <- data[data[[best_attribute]] == value, ]
if (nrow(subset_data) == 0) {
# No instances for this branch, return the majority class of the parent node
tree$children[[as.character(value)]] <- names(sort(table(data[[target]]), decreasing = TRUE)[1])
} else {
tree$children[[as.character(value)]] <- build_decision_tree(subset_data, target)
}
}
return(tree)
}
# Build the decision tree
decision_tree <- build_decision_tree(data, "Class")
# Install and load the 'igraph' library
library(igraph)
# Function to create a graph from the decision tree
create_tree_graph <- function(tree) {
g <- graph.empty(directed = TRUE)
create_node <- function(node, parent = NULL) {
nodeName <- node$attribute
if (!is.null(parent)) {
g <- add.edges(g, c(parent, nodeName))
}
for (child in names(node$children)) {
g <- create_node(node$children[[child]], parent = nodeName)
}
return(g)
}
return(create_node(tree))
}
# Create a graph from the decision tree
tree_graph <- create_tree_graph(decision_tree)
# Load necessary libraries
library(party)
library(caret)
library(e1071)
library(RColorBrewer)
# Load your breast cancer dataset from a CSV file
data <- read.csv("breast_cancer_data.csv")
# Load necessary libraries
library(party)
library(caret)
library(e1071)
library(RColorBrewer)
# Load your breast cancer dataset from a CSV file
data <- read.csv("BreastCancer.csv")
# Define a function to calculate information gain
information_gain <- function(data, attribute, class) {
# Calculate entropy for the entire dataset
entropy_total <- entropy(data$class)
# Calculate the weighted average entropy after splitting on the attribute
weighted_entropy <- 0
unique_values <- unique(data[[attribute]])
for (value in unique_values) {
subset_data <- data[data[[attribute]] == value, ]
weight <- nrow(subset_data) / nrow(data)
weighted_entropy <- weighted_entropy + weight * entropy(subset_data$class)
}
# Calculate information gain
information_gain <- entropy_total - weighted_entropy
return(information_gain)
}
# Define a function to calculate entropy
entropy <- function(class) {
probabilities <- table(class) / length(class)
entropy <- -sum(probabilities * log2(probabilities + 1e-10))
return(entropy)
}
# Define a recursive function to build the decision tree
build_tree <- function(data, depth) {
# Check for base cases
if (depth == 0 || length(unique(data$class)) == 1) {
return(leaf(data$class))
}
# Initialize variables to find the best attribute
best_attribute <- NULL
best_information_gain <- -Inf
# Find the attribute with the highest information gain
for (attribute in names(data)[names(data) != "class"]) {
gain <- information_gain(data, attribute, data$class)
if (gain > best_information_gain) {
best_information_gain <- gain
best_attribute <- attribute
}
}
# Create a decision node
node <- split_node(data, best_attribute)
# Recurse on the sublists obtained by splitting on the best attribute
unique_values <- unique(data[[best_attribute]])
for (value in unique_values) {
subset_data <- data[data[[best_attribute]] == value, ]
child <- build_tree(subset_data, depth - 1)
add_child(node, value, child)
}
return(node)
}
# Define a function to create a leaf node
leaf <- function(class) {
return(paste("Leaf:", names(sort(table(class), decreasing = TRUE)[1])))
}
# Define a function to create a split node
split_node <- function(data, attribute) {
node <- list()
node$attribute <- attribute
node$children <- list()
return(node)
}
# Define a function to add a child node to a split node
add_child <- function(node, value, child) {
node$children[[as.character(value)]] <- child
}
# Build the decision tree
tree <- build_tree(data, depth = 3) # You can adjust the depth as needed
# Print the decision tree
print(tree)
# You can plot the tree with different packages like 'party' or 'rpart.plot'
# Make predictions using the decision tree
predictions <- sapply(1:nrow(data), function(i) predict_tree(tree, data[i, ]))
# Create a confusion matrix
confusion_matrix <- table(Actual = data$class, Predicted = predictions)
